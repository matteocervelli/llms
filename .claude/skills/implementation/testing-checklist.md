# Testing Checklist

## Overview

This checklist ensures comprehensive test coverage for all feature implementations, targeting 80%+ code coverage with unit, integration, and end-to-end tests.

---

## Test Coverage Requirements

### Minimum Coverage Targets

- **Overall Code Coverage:** 80%+
- **Core Business Logic:** 90%+
- **Utilities and Helpers:** 85%+
- **Integration Points:** 75%+
- **CLI/Entry Points:** 70%+

### Coverage Verification

```bash
# Run tests with coverage report
pytest --cov=src --cov-report=html --cov-report=term-missing

# View HTML report
open htmlcov/index.html

# Check coverage threshold
pytest --cov=src --cov-fail-under=80
```

---

## Unit Testing

### Core Business Logic

- [ ] All public methods tested
- [ ] All branches and conditions tested
- [ ] Happy path scenarios covered
- [ ] Edge cases identified and tested
- [ ] Error conditions tested
- [ ] Boundary values tested

**Example:**
```python
def test_process_data_valid_input():
    """Test successful processing."""
    result = process_data({"name": "test", "value": 123})
    assert result.name == "test"

def test_process_data_empty_name():
    """Test empty name validation."""
    with pytest.raises(ValidationError):
        process_data({"name": "", "value": 123})

def test_process_data_negative_value():
    """Test negative value validation."""
    with pytest.raises(ValidationError):
        process_data({"name": "test", "value": -1})

def test_process_data_boundary_value():
    """Test maximum value boundary."""
    result = process_data({"name": "test", "value": 999999})
    assert result.value == 999999
```

### Data Models (Pydantic)

- [ ] Valid input creates model successfully
- [ ] Invalid input raises ValidationError
- [ ] Default values applied correctly
- [ ] Field validation rules enforced
- [ ] Custom validators tested
- [ ] Serialization/deserialization tested

**Example:**
```python
def test_user_model_valid():
    """Test valid user model creation."""
    user = UserModel(name="Alice", age=30, email="alice@example.com")
    assert user.name == "Alice"
    assert user.age == 30

def test_user_model_invalid_email():
    """Test invalid email validation."""
    with pytest.raises(ValidationError):
        UserModel(name="Alice", age=30, email="invalid-email")

def test_user_model_age_range():
    """Test age range validation."""
    with pytest.raises(ValidationError):
        UserModel(name="Alice", age=-1, email="alice@example.com")
```

### Validators

- [ ] Valid input passes validation
- [ ] Invalid input fails with clear error
- [ ] Multiple validation rules tested
- [ ] Validation error messages checked
- [ ] Edge cases (empty, null, extreme values) tested

### Repositories (with Mocks)

- [ ] Create operation tested
- [ ] Read operation tested
- [ ] Update operation tested
- [ ] Delete operation tested
- [ ] Not found scenarios tested
- [ ] Error handling tested
- [ ] Mock external dependencies

**Example:**
```python
@pytest.fixture
def mock_file_system(mocker):
    """Mock file system operations."""
    return mocker.patch("pathlib.Path.write_text")

def test_repository_save(mock_file_system):
    """Test repository save operation."""
    repo = FileRepository(Path("/fake"))
    feature = Feature(name="test", value=123)

    result = repo.save(feature)

    mock_file_system.assert_called_once()
    assert result.name == "test"
```

### Utilities and Helpers

- [ ] All utility functions tested
- [ ] Pure functions tested with various inputs
- [ ] Helper functions tested in isolation
- [ ] Stateless utilities don't have side effects
- [ ] String/data manipulation tested

---

## Integration Testing

### Service Integration

- [ ] Service integrates with repository correctly
- [ ] Service integrates with validator correctly
- [ ] Multiple components work together
- [ ] Data flows through layers correctly
- [ ] Dependency injection works as expected

**Example:**
```python
def test_service_create_integration(temp_dir):
    """Test service creation with real repository."""
    # Real repository (not mocked)
    repo = FileRepository(temp_dir)
    validator = FeatureValidator()
    service = FeatureService(repo, validator)

    # Create through service
    result = service.create(name="test", value=123)

    # Verify persistence
    retrieved = service.get(result.id)
    assert retrieved.name == "test"
```

### Database Integration (If Applicable)

- [ ] CRUD operations tested end-to-end
- [ ] Transactions tested (commit/rollback)
- [ ] Queries return expected results
- [ ] Foreign key constraints tested
- [ ] Unique constraints tested
- [ ] Database cleanup after tests

### File System Integration

- [ ] Files created correctly
- [ ] Files read correctly
- [ ] Files updated correctly
- [ ] Files deleted correctly
- [ ] Directory structure created correctly
- [ ] Cleanup after tests (use tmp_path fixture)

**Example:**
```python
def test_file_operations(tmp_path):
    """Test file system operations."""
    # Arrange
    file_path = tmp_path / "test.json"
    data = {"key": "value"}

    # Act: Write
    write_json(file_path, data)

    # Assert: File exists and contains data
    assert file_path.exists()
    assert read_json(file_path) == data

    # Cleanup happens automatically with tmp_path
```

### External API Integration (If Applicable)

- [ ] API calls return expected data
- [ ] Error responses handled correctly
- [ ] Timeouts handled correctly
- [ ] Retry logic tested
- [ ] Rate limiting tested
- [ ] Use mock server or VCR.py for tests

---

## End-to-End Testing

### CLI Testing

- [ ] All commands execute successfully
- [ ] Command-line arguments parsed correctly
- [ ] Output format correct
- [ ] Error messages clear and helpful
- [ ] Exit codes correct

**Example:**
```python
from click.testing import CliRunner

def test_cli_create_command():
    """Test create command via CLI."""
    runner = CliRunner()
    result = runner.invoke(cli, ["create", "--name", "test"])

    assert result.exit_code == 0
    assert "Created: test" in result.output

def test_cli_invalid_input():
    """Test CLI with invalid input."""
    runner = CliRunner()
    result = runner.invoke(cli, ["create", "--name", ""])

    assert result.exit_code != 0
    assert "Error" in result.output
```

### Workflow Testing

- [ ] Complete user workflows tested
- [ ] Multiple operations in sequence tested
- [ ] State changes verified
- [ ] Side effects validated

**Example:**
```python
def test_complete_workflow(temp_dir):
    """Test complete create-update-delete workflow."""
    service = create_service(temp_dir)

    # Create
    created = service.create(name="test", value=123)
    assert created.value == 123

    # Update
    updated = service.update(created.id, value=456)
    assert updated.value == 456

    # Read
    retrieved = service.get(created.id)
    assert retrieved.value == 456

    # Delete
    service.delete(created.id)
    with pytest.raises(NotFoundError):
        service.get(created.id)
```

---

## Test Organization

### Directory Structure

```
tests/
├── __init__.py
├── conftest.py           # Shared fixtures
├── unit/                 # Unit tests
│   ├── test_models.py
│   ├── test_validators.py
│   ├── test_core.py
│   └── test_utils.py
├── integration/          # Integration tests
│   ├── test_service.py
│   ├── test_repository.py
│   └── test_cli.py
└── fixtures/             # Test data
    ├── sample_data.json
    └── test_configs.py
```

### Test File Naming

- **Naming:** `test_*.py` or `*_test.py`
- **Mirror source:** `src/core.py` → `tests/unit/test_core.py`
- **One test file per source file** (for unit tests)

### Test Function Naming

```python
# Pattern: test_<function>_<condition>_<expected>
def test_validate_input_valid_data_returns_true():
    pass

def test_validate_input_empty_name_raises_error():
    pass

def test_process_data_large_input_handles_correctly():
    pass
```

---

## Fixtures

### Shared Fixtures (conftest.py)

```python
# tests/conftest.py
import pytest
from pathlib import Path

@pytest.fixture
def sample_data():
    """Sample data for tests."""
    return {
        "name": "test",
        "value": 123,
        "tags": ["tag1", "tag2"]
    }

@pytest.fixture
def temp_directory(tmp_path):
    """Temporary directory for test files."""
    test_dir = tmp_path / "test_data"
    test_dir.mkdir()
    yield test_dir
    # Cleanup happens automatically

@pytest.fixture
def mock_service(mocker):
    """Mocked service for testing."""
    service = mocker.Mock()
    service.create.return_value = Feature(id=1, name="test")
    return service
```

### Fixture Scopes

```python
# Function scope (default): New fixture per test
@pytest.fixture
def data():
    return create_data()

# Module scope: One fixture per module
@pytest.fixture(scope="module")
def expensive_resource():
    resource = create_expensive_resource()
    yield resource
    cleanup_resource(resource)

# Session scope: One fixture per test session
@pytest.fixture(scope="session")
def database():
    db = setup_database()
    yield db
    teardown_database(db)
```

---

## Mocking and Patching

### When to Mock

- External APIs
- File system operations (for unit tests)
- Database operations (for unit tests)
- Time-dependent functions
- Random number generation
- Expensive operations

### Mock Examples

```python
from unittest.mock import Mock, MagicMock, patch

# Mock object
def test_with_mock():
    """Test with mock object."""
    mock_repo = Mock()
    mock_repo.save.return_value = Feature(id=1, name="test")

    service = FeatureService(mock_repo)
    result = service.create("test")

    mock_repo.save.assert_called_once()
    assert result.name == "test"

# Patch function
@patch("module.external_function")
def test_with_patch(mock_function):
    """Test with patched function."""
    mock_function.return_value = "mocked"
    result = call_external_function()
    assert result == "mocked"

# Patch context manager
def test_with_patch_context():
    """Test with patch context manager."""
    with patch("module.function") as mock_func:
        mock_func.return_value = "mocked"
        result = call_function()
        assert result == "mocked"

# Mocker fixture (pytest-mock)
def test_with_mocker(mocker):
    """Test with mocker fixture."""
    mock = mocker.patch("module.function")
    mock.return_value = "mocked"
    result = call_function()
    assert result == "mocked"
```

---

## Parametrized Tests

### Multiple Input Cases

```python
import pytest

@pytest.mark.parametrize("input_val,expected", [
    ("valid", True),
    ("invalid", False),
    ("", False),
    ("test", True),
])
def test_validation(input_val, expected):
    """Test validation with multiple inputs."""
    result = validate(input_val)
    assert result == expected

@pytest.mark.parametrize("name,value", [
    ("test1", 100),
    ("test2", 200),
    ("test3", 300),
])
def test_create_with_values(name, value):
    """Test creation with different values."""
    result = create(name, value)
    assert result.name == name
    assert result.value == value
```

### Parametrize with IDs

```python
@pytest.mark.parametrize(
    "input_val,expected",
    [
        ("valid", True),
        ("invalid", False),
        ("", False),
    ],
    ids=["valid-input", "invalid-input", "empty-input"]
)
def test_validation(input_val, expected):
    """Test validation with labeled cases."""
    result = validate(input_val)
    assert result == expected
```

---

## Test Markers

### Custom Markers

```python
# pytest.ini or pyproject.toml
[tool.pytest.ini_options]
markers =
    slow: marks tests as slow
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    requires_network: marks tests requiring network

# Usage in tests
@pytest.mark.slow
def test_expensive_operation():
    """Slow test."""
    pass

@pytest.mark.integration
def test_database_integration():
    """Integration test."""
    pass

# Run specific markers
pytest -m "not slow"           # Skip slow tests
pytest -m integration          # Run only integration tests
pytest -m "unit and not slow"  # Run fast unit tests
```

### Built-in Markers

```python
# Skip test
@pytest.mark.skip(reason="Not implemented yet")
def test_future_feature():
    pass

# Skip conditionally
@pytest.mark.skipif(sys.version_info < (3, 9), reason="Requires Python 3.9+")
def test_new_syntax():
    pass

# Expected to fail
@pytest.mark.xfail(reason="Known bug #123")
def test_known_issue():
    pass
```

---

## Assertion Best Practices

### Clear Assertions

```python
# Good: Clear, specific assertions
assert result.name == "expected"
assert result.value > 0
assert len(result.items) == 3

# Avoid: Complex assertions
assert result == expected and result.valid and len(result.items) > 0  # ❌
```

### Assertion Messages

```python
# Good: Helpful assertion messages
assert result.name == "expected", f"Expected 'expected', got '{result.name}'"

# Good: pytest-specific assertions
assert result.name == "expected"  # pytest provides detailed output
```

### pytest Assertions

```python
# Approximate comparisons
assert result == pytest.approx(3.14, rel=0.01)

# Exception assertions
with pytest.raises(ValueError, match="Invalid input"):
    process_invalid_data()

# Warning assertions
with pytest.warns(DeprecationWarning):
    deprecated_function()
```

---

## Performance Testing

### Response Time Tests

```python
import time
import pytest

def test_performance_requirement():
    """Test meets performance requirement."""
    start = time.time()

    # Operation to test
    result = expensive_operation()

    duration = time.time() - start

    assert duration < 1.0, f"Operation took {duration}s, expected < 1.0s"
    assert result is not None
```

### Benchmarking (pytest-benchmark)

```python
def test_benchmark_operation(benchmark):
    """Benchmark operation performance."""
    result = benchmark(function_to_benchmark, arg1, arg2)
    assert result is not None
```

---

## Test Data Management

### Test Fixtures (Files)

```
tests/fixtures/
├── sample_input.json
├── expected_output.json
└── test_config.yaml
```

```python
from pathlib import Path

FIXTURES_DIR = Path(__file__).parent / "fixtures"

def test_with_fixture_file():
    """Test using fixture file."""
    input_data = json.loads((FIXTURES_DIR / "sample_input.json").read_text())
    result = process(input_data)
    expected = json.loads((FIXTURES_DIR / "expected_output.json").read_text())
    assert result == expected
```

### Factory Functions

```python
def create_test_user(name="test", age=30, **kwargs):
    """Factory function for test users."""
    defaults = {"name": name, "age": age, "email": f"{name}@test.com"}
    defaults.update(kwargs)
    return User(**defaults)

def test_with_factory():
    """Test using factory function."""
    user1 = create_test_user()
    user2 = create_test_user(name="alice", age=25)
    assert user1.name == "test"
    assert user2.name == "alice"
```

---

## Continuous Integration

### Test Commands

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html --cov-report=term

# Run specific tests
pytest tests/unit/
pytest tests/integration/test_service.py
pytest tests/unit/test_core.py::test_specific_function

# Run with markers
pytest -m "not slow"
pytest -m integration

# Verbose output
pytest -v

# Stop on first failure
pytest -x

# Run last failed tests
pytest --lf

# Parallel execution
pytest -n auto  # Requires pytest-xdist
```

### CI Configuration Example

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: "3.9"
      - run: pip install -r requirements.txt
      - run: pip install -e ".[dev]"
      - run: pytest --cov=src --cov-report=xml --cov-fail-under=80
      - uses: codecov/codecov-action@v2
        with:
          file: ./coverage.xml
```

---

## Testing Checklist Summary

### Before Marking Feature Complete

**Unit Tests:**
- [ ] All public functions/methods tested
- [ ] All branches and conditions covered
- [ ] Edge cases and boundaries tested
- [ ] Error conditions tested
- [ ] Mocks used for dependencies

**Integration Tests:**
- [ ] Service layers integrated and tested
- [ ] File/database operations tested end-to-end
- [ ] External dependencies tested (mocked or sandboxed)

**Coverage:**
- [ ] Overall coverage ≥ 80%
- [ ] Core business logic ≥ 90%
- [ ] No critical paths untested

**Test Quality:**
- [ ] Tests are independent (no test order dependency)
- [ ] Tests clean up after themselves
- [ ] Test names are clear and descriptive
- [ ] Assertions have helpful messages
- [ ] Tests run quickly (< 1 min for unit tests)

**CI/CD:**
- [ ] Tests pass in CI pipeline
- [ ] Coverage report generated
- [ ] No flaky tests

---

## References

- [pytest Documentation](https://docs.pytest.org/)
- [Python Testing Best Practices](https://realpython.com/pytest-python-testing/)
- [Effective Python Testing](https://testdriven.io/blog/testing-python/)
- [Mock Documentation](https://docs.python.org/3/library/unittest.mock.html)
